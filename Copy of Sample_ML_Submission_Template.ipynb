{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vncDsAP0Gaoa"
   },
   "source": [
    "# **Project Name**    -\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "beRrZCGUAJYm"
   },
   "source": [
    "##### **Project Type**    - EDA/Regression/Classification/Unsupervised\n",
    "##### **Contribution**    - Individual/Team\n",
    "##### **Team Member 1 -**\n",
    "##### **Team Member 2 -**\n",
    "##### **Team Member 3 -**\n",
    "##### **Team Member 4 -**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FJNUwmbgGyua"
   },
   "source": [
    "# **Project Summary -**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F6v_1wHtG2nS"
   },
   "source": [
    "Write the summary here within 500-600 words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w6K7xa23Elo4"
   },
   "source": [
    "# **GitHub Link -**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h1o69JH3Eqqn"
   },
   "source": [
    "Provide your GitHub Link here.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yQaldy8SH6Dl"
   },
   "source": [
    "# **Problem Statement**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DpeJGUA3kjGy"
   },
   "source": [
    "**Write Problem Statement Here.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mDgbUHAGgjLW"
   },
   "source": [
    "# **General Guidelines** : -  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZrxVaUj-hHfC"
   },
   "source": [
    "1.   Well-structured, formatted, and commented code is required.\n",
    "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
    "     \n",
    "     The additional credits will have advantages over other students during Star Student selection.\n",
    "       \n",
    "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
    "                       without a single error logged. ]\n",
    "\n",
    "3.   Each and every logic should have proper comments.\n",
    "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
    "        \n",
    "\n",
    "```\n",
    "# Chart visualization code\n",
    "```\n",
    "            \n",
    "\n",
    "*   Why did you pick the specific chart?\n",
    "*   What is/are the insight(s) found from the chart?\n",
    "* Will the gained insights help creating a positive business impact?\n",
    "Are there any insights that lead to negative growth? Justify with specific reason.\n",
    "\n",
    "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
    "\n",
    "\n",
    "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
    "\n",
    "U - Univariate Analysis,\n",
    "\n",
    "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
    "\n",
    "M - Multivariate Analysis\n",
    " ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
    "\n",
    "\n",
    "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
    "\n",
    "\n",
    "*   Cross- Validation & Hyperparameter Tuning\n",
    "\n",
    "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
    "\n",
    "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O_i_v8NEhb9l"
   },
   "source": [
    "# ***Let's Begin !***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HhfV-JJviCcP"
   },
   "source": [
    "## ***1. Know Your Data***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y3lxredqlCYt"
   },
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M8Vqi-pPk-HR"
   },
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from google.colab import drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3RnN4peoiCZX"
   },
   "source": [
    "### Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4CkvbW_SlZ_R"
   },
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "df = pd.read_csv(\"/home/rohanr/Documents/Projects/Project 1//Customer_support_data.csv\")\n",
    "df_original = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x71ZqKXriCWQ"
   },
   "source": [
    "### Dataset First View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LWNFOSvLl09H"
   },
   "outputs": [],
   "source": [
    "# Dataset First Look\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7hBIi_osiCS2"
   },
   "source": [
    "### Dataset Rows & Columns count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kllu7SJgmLij"
   },
   "outputs": [],
   "source": [
    "# Dataset Rows & Columns count\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JlHwYmJAmNHm"
   },
   "source": [
    "### Dataset Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e9hRXRi6meOf"
   },
   "outputs": [],
   "source": [
    "# Dataset Info\n",
    "df_original.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "35m5QtbWiB9F"
   },
   "source": [
    "#### Duplicate Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1sLdpKYkmox0"
   },
   "outputs": [],
   "source": [
    "# Dataset Duplicate Value Count\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PoPl-ycgm1ru"
   },
   "source": [
    "#### Missing Values/Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GgHWkxvamxVg"
   },
   "outputs": [],
   "source": [
    "# Missing Values/Null Values Count\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3q5wnI3om9sJ"
   },
   "outputs": [],
   "source": [
    "# Visualizing the missing values\n",
    "sns.heatmap(df.isnull())\n",
    "plt.title(\"Missing values\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H0kj-8xxnORC"
   },
   "source": [
    "### What did you know about your dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gfoNAAC-nUe_"
   },
   "source": [
    "It is showing a huge data of customers who have bought some products and hwat was their overall response from the product. The table also shows about customer's city , date he bought the product , what kind of product category does it fall in and who was the agent by whom the order was forwarded and his manager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nA9Y7ga8ng1Z"
   },
   "source": [
    "## ***2. Understanding Your Variables***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j7xfkqrt5Ag5"
   },
   "outputs": [],
   "source": [
    "# Dataset Columns\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DnOaZdaE5Q5t"
   },
   "outputs": [],
   "source": [
    "# Dataset Describe\n",
    "df.info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PBTbrJXOngz2"
   },
   "source": [
    "### Variables Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aJV4KIxSnxay"
   },
   "source": [
    "Their are multiple columns describing about teh vcustomer like id , date of order , category of product ordered etc.It also has CSAT score for EDA analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u3PMJOP6ngxN"
   },
   "source": [
    "### Check Unique Values for each variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zms12Yq5n-jE"
   },
   "outputs": [],
   "source": [
    "# Check Unique Values for each variable.\n",
    "df.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dauF4eBmngu3"
   },
   "source": [
    "## 3. ***Data Wrangling***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bKJF3rekwFvQ"
   },
   "source": [
    "### Data Wrangling Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wk-9a2fpoLcV"
   },
   "outputs": [],
   "source": [
    "# Data Wrangling: Make dataset analysis-ready\n",
    "\n",
    "# Fill or leaves missing values\n",
    "df['CSAT Score'].fillna(df['CSAT Score'].median(), inplace=True)\n",
    "df.dropna(subset=['Customer_City', 'Product_category'], inplace=True)\n",
    "# Renaming columns\n",
    "df.rename(columns=lambda x: x.strip().lower().replace(' ', '_'), inplace=True)\n",
    "#Removes duplicates\n",
    "df.drop_duplicates(inplace=True)\n",
    "# Converting date column to date time format\n",
    "df['order_date_time'] = pd.to_datetime(df['order_date_time'], errors='coerce')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MSa1f5Uengrz"
   },
   "source": [
    "### What all manipulations have you done and insights you found?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LbyXE7I1olp8"
   },
   "source": [
    "Convert order date time and survey date time to actual date time format Filled the missing values of CSAT score with the overall median of the column. Dropped the missing values of Customer city and Product Category Finally removed overall duplicates from the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GF8Ens_Soomf"
   },
   "source": [
    "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0wOQAZs5pc--"
   },
   "source": [
    "#### Chart - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7v_ESjsspbW7"
   },
   "outputs": [],
   "source": [
    "# Chart - 1 visualization code\n",
    "sns.countplot(data=df,y='product_category',palette='plasma')\n",
    "plt.title('Orders per category')\n",
    "plt.xticks(rotation=90) #Helps to rotate the text at x axis\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K5QZ13OEpz2H"
   },
   "source": [
    "##### 1. Why did you pick the specific chart?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XESiWehPqBRc"
   },
   "source": [
    "Just wanted to know the how many different categories are their of product and approximate how many products have been sold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lQ7QKXXCp7Bj"
   },
   "source": [
    "##### 2. What is/are the insight(s) found from the chart?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C_j1G7yiqdRP"
   },
   "source": [
    "Electronic items have been in most demand and giftcards are in least demand.Lifestyle products and mobile have a decent selling . Other than gift cards , affiliates and furnitures also comes in very less demand. Remaining products are usually sold on average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "448CDAPjqfQr"
   },
   "source": [
    "##### 3. Will the gained insights help creating a positive business impact?\n",
    "Are there any insights that lead to negative growth? Justify with specific reason."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3cspy4FjqxJW"
   },
   "source": [
    "Electronic items are the most demanded, while gift cards, affiliates, and furniture have very low sales. Focusing on high-demand products can boost revenue, but continuing to invest in low-demand categories will lead to a quite loss. The reasoon is quite clear from the data shown above. Electronic items can provide a good profit overall . But furniture and giftcards may cause a loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KSlN3yHqYklG"
   },
   "source": [
    "#### Chart - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R4YgtaqtYklH"
   },
   "outputs": [],
   "source": [
    "# Chart - 2 visualization code\n",
    "sns.barplot(data=df , x='product_category' , y= 'csat_score' , palette = 'Set2' , ci=None)\n",
    "plt.title('CSAT Score as per category')\n",
    "plt.xticks(rotation = 90)\n",
    "plt.ylabel(\"Aerage CART Score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hP0yY5QBFYDQ"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t6dVpIINYklI"
   },
   "source": [
    "##### 1. Why did you pick the specific chart?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5aaW0BYyYklI"
   },
   "source": [
    "To compare the overall CSAT score with the product category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ijmpgYnKYklI"
   },
   "source": [
    "##### 2. What is/are the insight(s) found from the chart?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PSx9atu2YklI"
   },
   "source": [
    "Average CSAT score of products of each product category is almost at a good range and more than 3.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-JiQyfWJYklI"
   },
   "source": [
    "##### 3. Will the gained insights help creating a positive business impact?\n",
    "Are there any insights that lead to negative growth? Justify with specific reason."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BcBbebzrYklV"
   },
   "source": [
    "Yes , it became prominent that all products have a good csat score overall so their are very less chances of loss of keeping any product but if a strict comparision is needed to be made then highest profitable prodcut can be Affliates and least profitable would be Giftcard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EM7whBJCYoAo"
   },
   "source": [
    "#### Chart - 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t6GMdE67YoAp"
   },
   "outputs": [],
   "source": [
    "# Chart - 3 visualization code\n",
    "sns.boxplot(data = df , x = 'agent_shift' , y = 'csat_score' , palette = 'pastel')\n",
    "plt.title('CSAT Score as per Agent Shift')\n",
    "plt.ylabel('CSAT Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fge-S5ZAYoAp"
   },
   "source": [
    "##### 1. Why did you pick the specific chart?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5dBItgRVYoAp"
   },
   "source": [
    "In order to visualize the distribution of CSAT scores for different agent shifts , this chart has been chosen . It shows which shift has most ocnsistent customer satisfaction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "85gYPyotYoAp"
   },
   "source": [
    "##### 2. What is/are the insight(s) found from the chart?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4jstXR6OYoAp"
   },
   "source": [
    "The box plot shows the Morning and Evening shifts have the most consistent CSAT scores, while the Afternoon and Night shifts are more variable. The Split shift has the widest range of performance, but all shifts maintain high median scores overall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RoGjAbkUYoAp"
   },
   "source": [
    "##### 3. Will the gained insights help creating a positive business impact?\n",
    "Are there any insights that lead to negative growth? Justify with specific reason."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zfJ8IqMcYoAp"
   },
   "source": [
    "Yes, the gained insights can drive positive business impact by helping managers improve agent performance through targeted training. However, the chart itself doesn't show negative growth, but the outliers and high variability in certain shifts could lead to a negative impact if not addressed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Of9eVA-YrdM"
   },
   "source": [
    "#### Chart - 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "irlUoxc8YrdO"
   },
   "outputs": [],
   "source": [
    "# Chart - 4 visualization code\n",
    "sns.barplot(data=df,x='product_category',y='connected_handling_time',palette='deep',ci=None)\n",
    "plt.xlabel('product_category')\n",
    "plt.ylabel('connected_handling_time')\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"Average handling time\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iky9q4vBYrdO"
   },
   "source": [
    "##### 1. Why did you pick the specific chart?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aJRCwT6DYrdO"
   },
   "source": [
    "It shows how much time does an agent spents on handling customer issues on that product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F6T5p64dYrdO"
   },
   "source": [
    "##### 2. What is/are the insight(s) found from the chart?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xx8WAJvtYrdO"
   },
   "source": [
    "Mobile issues take the most handling time (~2000), meaning mobiles need the most support effort. Lifestyle, electronics, books, and home appliances require much less—about 30% of mobile’s time—indicating moderate complexity. Giftcards, affiliates, furniture, and home show negligible handling time, suggesting either very low demand or minimal support needs. This highlights mobiles as the key area for improving efficiency or allocating more resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y-Ehk30pYrdP"
   },
   "source": [
    "##### 3. Will the gained insights help creating a positive business impact?\n",
    "Are there any insights that lead to negative growth? Justify with specific reason."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jLNxxz7MYrdP"
   },
   "source": [
    "Yes, the insight can drive positive impact by focusing resources on mobiles, improving efficiency where support demand is highest. Negatively, if mobile handling time stays excessive, it may slow service and increase costs, risking customer dissatisfaction and reduced profitability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bamQiAODYuh1"
   },
   "source": [
    "#### Chart - 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TIJwrbroYuh3"
   },
   "outputs": [],
   "source": [
    "# Chart - 5 visualization code\n",
    "orderCount = df['category'].value_counts()\n",
    "plt.pie(orderCount,labels=orderCount.index , autopct='%1.2f%%')\n",
    "plt.title('Order share as per category')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QHF8YVU7Yuh3"
   },
   "source": [
    "##### 1. Why did you pick the specific chart?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dcxuIMRPYuh3"
   },
   "source": [
    "In order to know the product category's share of total orders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GwzvFGzlYuh3"
   },
   "source": [
    "##### 2. What is/are the insight(s) found from the chart?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uyqkiB8YYuh3"
   },
   "source": [
    "About 50% of orders were returned , Almost 35% were just regarding orders , somewere for refunds , cancellation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qYpmQ266Yuh3"
   },
   "source": [
    "##### 3. Will the gained insights help creating a positive business impact?\n",
    "Are there any insights that lead to negative growth? Justify with specific reason."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_WtzZ_hCYuh4"
   },
   "source": [
    "It became very clear that overall we are at a very less profit because more than 50% were refunded in which many were ordered cancelled . Only 34..7% , including a feeback contact of 3% proves that very less profit has been gained overall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OH-pJp9IphqM"
   },
   "source": [
    "#### Chart - 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kuRf4wtuphqN"
   },
   "outputs": [],
   "source": [
    "# Chart - 6 visualization code\n",
    "orderCount = df['product_category'].value_counts()\n",
    "plt.pie(orderCount , labels = orderCount.index , autopct = '%1.2f%%')\n",
    "plt.title(\"Order share are per product category\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bbFf2-_FphqN"
   },
   "source": [
    "##### 1. Why did you pick the specific chart?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "loh7H2nzphqN"
   },
   "source": [
    "To know overall buisness is based on which category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ouA3fa0phqN"
   },
   "source": [
    "##### 2. What is/are the insight(s) found from the chart?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VECbqPI7phqN"
   },
   "source": [
    "Electronics and Lifestyle together cover about 50% of whole buisness . Books and mobile cover about 30% of total buisness and rest are the remaining products with least sales overall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Seke61FWphqN"
   },
   "source": [
    "##### 3. Will the gained insights help creating a positive business impact?\n",
    "Are there any insights that lead to negative growth? Justify with specific reason."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DW4_bGpfphqN"
   },
   "source": [
    "Electronics and lifestyle products have a good scope in future for growth and high profits . Mobile products and books can be good for side buisness , so overall electronics and lifestyle products scope must be increased in the market and few products like giftcards and affiliates can be ignored , they may not affect much in the profit of overall buisness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PIIx-8_IphqN"
   },
   "source": [
    "#### Chart - 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lqAIGUfyphqO"
   },
   "outputs": [],
   "source": [
    "# Chart - 7 visualization code\n",
    "avgScore = df.groupby('agent_name')['csat_score'].mean()\n",
    "scoreGroup = avgScore.round()\n",
    "scoreCount = scoreGroup.value_counts().sort_index()\n",
    "\n",
    "plt.bar(scoreCount.index.astype(str),scoreCount.values,color='teal')\n",
    "plt.title(\"Average Agent CSAT Score\")\n",
    "plt.xlabel(\"Average CSAT score\")\n",
    "plt.ylabel(\"Number of Agents\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t27r6nlMphqO"
   },
   "source": [
    "##### 1. Why did you pick the specific chart?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iv6ro40sphqO"
   },
   "source": [
    "To know that overall depending of csat score what kind of relation may fit with the agent assigned to them because selling of product depend very much on agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r2jJGEOYphqO"
   },
   "source": [
    "##### 2. What is/are the insight(s) found from the chart?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Po6ZPi4hphqO"
   },
   "source": [
    "Most of the agents fall in the CSAT score group of 4 which has gone alomost about 800"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b0JNsNcRphqO"
   },
   "source": [
    "##### 3. Will the gained insights help creating a positive business impact?\n",
    "Are there any insights that lead to negative growth? Justify with specific reason."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xvSq8iUTphqO"
   },
   "source": [
    "From this chart we came to know that may be about 50-60 agents who belong to range of either 1 or 2 need more training or are not interested to work but on other hand agents with CSAT score of 3 just need to brush up their skills for a better performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BZR9WyysphqO"
   },
   "source": [
    "#### Chart - 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TdPTWpAVphqO"
   },
   "outputs": [],
   "source": [
    "# Chart - 8 visualization code\n",
    "df['year'] = df['order_date_time'].dt.to_period('Y')\n",
    "monthlySales = df.groupby('year')['order_id'].count()\n",
    "monthlySales.plot(kind='bar',color='skyblue')\n",
    "plt.title('Yearly Sales Trend')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Number of Orders')\n",
    "plt.xticks(rotation = 45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jj7wYXLtphqO"
   },
   "source": [
    "##### 1. Why did you pick the specific chart?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ob8u6rCTphqO"
   },
   "source": [
    "Just to know , overall how much sales were their in a complete year, Due to only sales of January in 2024 , it has not been mentioned in the graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eZrbJ2SmphqO"
   },
   "source": [
    "##### 2. What is/are the insight(s) found from the chart?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mZtgC_hjphqO"
   },
   "source": [
    "In 2022 , their was almost no sales , but in 2023 sales reached great heights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rFu4xreNphqO"
   },
   "source": [
    "##### 3. Will the gained insights help creating a positive business impact?\n",
    "Are there any insights that lead to negative growth? Justify with specific reason."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ey_0qi68phqO"
   },
   "source": [
    "Well not exactly but yes the gaph shows that buisness is gaining popularity which is important thing , the graph also rises a hopes for a better performance in year 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YJ55k-q6phqO"
   },
   "source": [
    "#### Chart - 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B2aS4O1ophqO"
   },
   "outputs": [],
   "source": [
    "# Chart - 9 visualization code\n",
    "topCity = df['customer_city'].value_counts().head(8)\n",
    "plt.bar(topCity.index,topCity.values,color='green')\n",
    "plt.title('Cities with highest orders')\n",
    "plt.xlabel('City')\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel(\"Order frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gCFgpxoyphqP"
   },
   "source": [
    "##### 1. Why did you pick the specific chart?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TVxDimi2phqP"
   },
   "source": [
    "To know the cities from where the buisness gets most customers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OVtJsKN_phqQ"
   },
   "source": [
    "##### 2. What is/are the insight(s) found from the chart?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ngGi97qjphqQ"
   },
   "source": [
    "Hyderabad and New Delhi are the places to provide highest number or sales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lssrdh5qphqQ"
   },
   "source": [
    "##### 3. Will the gained insights help creating a positive business impact?\n",
    "Are there any insights that lead to negative growth? Justify with specific reason."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tBpY5ekJphqQ"
   },
   "source": [
    "Not much it is just to get an idea that from which city are we receiving highest amount of sales , it also suggest us to do more investment for these specific areas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U2RJ9gkRphqQ"
   },
   "source": [
    "#### Chart - 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GM7a4YP4phqQ"
   },
   "outputs": [],
   "source": [
    "# Chart - 10 visualization code\n",
    "df['month'] = df['order_date_time'].dt.to_period('M')\n",
    "monthlySales = df.groupby('month').size()\n",
    "plt.plot(monthlySales.index.astype(str),monthlySales.values,marker='o')\n",
    "plt.title('Monthly Sales Trend')\n",
    "plt.xlabel('Month')\n",
    "plt.xticks(rotation = 60)\n",
    "plt.ylabel('Order frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1M8mcRywphqQ"
   },
   "source": [
    "##### 1. Why did you pick the specific chart?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8agQvks0phqQ"
   },
   "source": [
    "Just to know the sales month wise in more clearer manner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tgIPom80phqQ"
   },
   "source": [
    "##### 2. What is/are the insight(s) found from the chart?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qp13pnNzphqQ"
   },
   "source": [
    "The main buisness and profit which company made throughout the tenure was in months of June , July and August of 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JMzcOPDDphqR"
   },
   "source": [
    "##### 3. Will the gained insights help creating a positive business impact?\n",
    "Are there any insights that lead to negative growth? Justify with specific reason."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R4Ka1PC2phqR"
   },
   "source": [
    "This helped us to know that their was a huge hike in particularly rainy season of 2023 but rest all time , we have not done any buisness so it is important to know and understand the factors for such an extraordinary performance in those particular months of that year."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x-EpHcCOp1ci"
   },
   "source": [
    "#### Chart - 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mAQTIvtqp1cj"
   },
   "outputs": [],
   "source": [
    "# Chart - 11 visualization code\n",
    "plt.scatter(df['csat_score'],df['item_price'],alpha=0.5,color='purple')\n",
    "plt.title('Item Price vs CSAT score')\n",
    "plt.ylabel('Item Price')\n",
    "plt.xlabel('CSAT Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X_VqEhTip1ck"
   },
   "source": [
    "##### 1. Why did you pick the specific chart?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-vsMzt_np1ck"
   },
   "source": [
    "In order to visually explore the relationship between CSAT scores and item price. A scatter plot is the most effective way to show if there's a correlation or a pattern between two numerical variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8zGJKyg5p1ck"
   },
   "source": [
    "##### 2. What is/are the insight(s) found from the chart?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZYdMsrqVp1ck"
   },
   "source": [
    "The chart reveals there's no clear linear relationship between an item's price and the customer's satisfaction score. While some high-priced items receive low CSAT scores, a wide range of prices, including high ones, also receive top scores of 4 and 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PVzmfK_Ep1ck"
   },
   "source": [
    "##### 3. Will the gained insights help creating a positive business impact?\n",
    "Are there any insights that lead to negative growth? Justify with specific reason."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "druuKYZpp1ck"
   },
   "source": [
    "A positive impact can be achieved by knowing that high prices don't inherently cause low satisfaction, allowing the business to confidently sell expensive items. The chart doesn't show an insight that directly leads to negative growth, but it does show that even high-priced items can fail to satisfy customers, which could negatively impact the brand if not addressed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n3dbpmDWp1ck"
   },
   "source": [
    "#### Chart - 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bwevp1tKp1ck"
   },
   "outputs": [],
   "source": [
    "# Chart - 12 visualization code\n",
    "avgScoreManager = df.groupby('manager')['csat_score'].mean()\n",
    "plt.bar(avgScoreManager.index,avgScoreManager.values,color='coral')\n",
    "plt.title('Managers according to CSAT Score')\n",
    "plt.xlabel('Manager')\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel('Average CSAT Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ylSl6qgtp1ck"
   },
   "source": [
    "##### 1. Why did you pick the specific chart?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m2xqNkiQp1ck"
   },
   "source": [
    "Just to have an overall idea if any manager performanceis affecting a team or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZWILFDl5p1ck"
   },
   "source": [
    "##### 2. What is/are the insight(s) found from the chart?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x-lUsV2mp1ck"
   },
   "source": [
    "All the managers have a good score implying that they have been handling their team with good efforts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M7G43BXep1ck"
   },
   "source": [
    "##### 3. Will the gained insights help creating a positive business impact?\n",
    "Are there any insights that lead to negative growth? Justify with specific reason."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5wwDJXsLp1cl"
   },
   "source": [
    "Answer Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ag9LCva-p1cl"
   },
   "source": [
    "#### Chart - 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EUfxeq9-p1cl"
   },
   "outputs": [],
   "source": [
    "# Chart - 13 visualization code\n",
    "shifts = df['agent_shift'].value_counts()\n",
    "plt.pie(shifts,labels=shifts.index,autopct='%1.1f%%')\n",
    "plt.title('Agent Shift Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E6MkPsBcp1cl"
   },
   "source": [
    "##### 1. Why did you pick the specific chart?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V22bRsFWp1cl"
   },
   "source": [
    "To quickly compare how orders are distributed across different agent shifts and spot which shift handles the most workload."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2cELzS2fp1cl"
   },
   "source": [
    "##### 2. What is/are the insight(s) found from the chart?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ozQPc2_Ip1cl"
   },
   "source": [
    "Highest workload is in daytime and least workload is in night , morning and evening is the most productive period for company"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3MPXvC8up1cl"
   },
   "source": [
    "##### 3. Will the gained insights help creating a positive business impact?\n",
    "Are there any insights that lead to negative growth? Justify with specific reason."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GL8l1tdLp1cl"
   },
   "source": [
    "Most orders happen in morning (49.3%) and evening (37.8%), so staffing those shifts strategically can boost efficiency and service quality—this is a positive impact. Low activity in night (1%) and split (4%) suggests maintaining many resources there could waste costs, which risks negative growth if not optimized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NC_X3p0fY2L0"
   },
   "source": [
    "#### Chart - 14 - Correlation Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xyC9zolEZNRQ"
   },
   "outputs": [],
   "source": [
    "# Correlation Heatmap visualization code\n",
    "nmb = df.select_dtypes(include=['number'])\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.heatmap(nmb.corr(), annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Between Numeric Variables')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UV0SzAkaZNRQ"
   },
   "source": [
    "##### 1. Why did you pick the specific chart?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DVPuT8LYZNRQ"
   },
   "source": [
    "This heatmap was chosen to quickly understand the relationships between variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YPEH6qLeZNRQ"
   },
   "source": [
    "##### 2. What is/are the insight(s) found from the chart?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bfSqtnDqZNRR"
   },
   "source": [
    "Insights show higher item prices and longer handling times weakly correlate with lower CSAT scores, while price and handling time are positively linked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q29F0dvdveiT"
   },
   "source": [
    "#### Chart - 15 - Pair Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o58-TEIhveiU"
   },
   "outputs": [],
   "source": [
    "# Pair Plot visualization code\n",
    "sns.pairplot(df)\n",
    "plt.suptitle('Pair plot of numeric variables')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EXh0U9oCveiU"
   },
   "source": [
    "##### 1. Why did you pick the specific chart?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eMmPjTByveiU"
   },
   "source": [
    "To visualize relationships and patterns between multiple numeric variables at once, helping spot correlations, clusters, or trends that single-variable charts might miss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "22aHeOlLveiV"
   },
   "source": [
    "##### 2. What is/are the insight(s) found from the chart?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uPQ8RGwHveiV"
   },
   "source": [
    "It gives a quite similar insigt as like the heatmap made just above showing the most important factors and showing the relationship between them i.e. csat score , average handling time and item price."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g-ATYxFrGrvw"
   },
   "source": [
    "## ***5. Hypothesis Testing***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yfr_Vlr8HBkt"
   },
   "source": [
    "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-7MS06SUHkB-"
   },
   "source": [
    "Answer Here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8yEUt7NnHlrM"
   },
   "source": [
    "### Hypothetical Statement - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tEA2Xm5dHt1r"
   },
   "source": [
    "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HI9ZP0laH0D-"
   },
   "source": [
    "Null hypothesis : Their is no significant difference ini the mean CSAT scores between Morning and evening shifts\n",
    "Alternate Hypothesis : Their is a significant difference between the mean of CSAT score of Morning and Evening shifts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ye_GrbUbZt6h"
   },
   "outputs": [],
   "source": [
    "df = df_original.copy()\n",
    "df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_')\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I79__PHVH19G"
   },
   "source": [
    "#### 2. Perform an appropriate statistical test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oZrfquKtyian"
   },
   "outputs": [],
   "source": [
    "# Perform Statistical Test to obtain P-Value\n",
    "from scipy.stats import ttest_ind\n",
    "#Comparion of morning and evening shifts with csat score\n",
    "sun = df[df['agent_shift']=='Morning']['csat_score']\n",
    "eve = df[df['agent_shift']=='Evening']['csat_score']\n",
    "#t test\n",
    "t , p = ttest_ind(sun,eve , equal_var=False)  # t_stat and P-value\n",
    "print(\"T statisticss : \",t)\n",
    "print(\"P statistics : \",p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ou-I18pAyIpj"
   },
   "source": [
    "##### Which statistical test have you done to obtain P-Value?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s2U0kk00ygSB"
   },
   "source": [
    "Independent two sample t test have been conducted using scipy.stats.ttest_ind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fF3858GYyt-u"
   },
   "source": [
    "##### Why did you choose the specific statistical test?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HO4K0gP5y3B4"
   },
   "source": [
    "To compare 2 independent groups Morning and Evening shift CSAT score , this helped to conclude that the data is continuos and approximately normally distributed , The test also determines if their average differ significantly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4_0_7-oCpUZd"
   },
   "source": [
    "### Hypothetical Statement - 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hwyV_J3ipUZe"
   },
   "source": [
    "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FnpLGJ-4pUZe"
   },
   "source": [
    "Null hypothesis : Average item price is same for Eledctronics and Lifestyle\n",
    "\n",
    "Alternate Hypothesis : Average item price differs between Electronic and Lifestyle category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jAhdP5mGeS9R"
   },
   "source": [
    "df.shape()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3yB-zSqbpUZe"
   },
   "source": [
    "#### 2. Perform an appropriate statistical test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sWxdNTXNpUZe"
   },
   "outputs": [],
   "source": [
    "# Perform Statistical Test to obtain P-Value\n",
    "electronics_time = pd.to_numeric(df[df['product_category']=='Electronics']['connected_handling_time'], errors='coerce').dropna()\n",
    "mobiles_time = pd.to_numeric(df[df['product_category']=='Mobile']['connected_handling_time'], errors='coerce').dropna()\n",
    "\n",
    "t2, p2 = ttest_ind(electronics_time, mobiles_time, equal_var=False)\n",
    "print(\"T-statistic:\", t2)\n",
    "print(\"P-value:\", p2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dEUvejAfpUZe"
   },
   "source": [
    "##### Which statistical test have you done to obtain P-Value?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oLDrPz7HpUZf"
   },
   "source": [
    "independent two sample t test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fd15vwWVpUZf"
   },
   "source": [
    "##### Why did you choose the specific statistical test?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4xOGYyiBpUZf"
   },
   "source": [
    "One or both th groups have too few values or all identical values , so the t test cannot compute statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bn_IUdTipZyH"
   },
   "source": [
    "### Hypothetical Statement - 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "49K5P_iCpZyH"
   },
   "source": [
    "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7gWI5rT9pZyH"
   },
   "source": [
    "Null Hypothesis : The mean CSAT scores are the same for all managers.\n",
    "\n",
    "Alternate Hypothesis: At least one manager has a different mean CSAT score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nff-vKELpZyI"
   },
   "source": [
    "#### 2. Perform an appropriate statistical test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s6AnJQjtpZyI"
   },
   "outputs": [],
   "source": [
    "# Perform Statistical Test to obtain P-Value\n",
    "from scipy.stats import f_oneway\n",
    "\n",
    "#Group csat score of manager\n",
    "mngr = df['manager'].dropna().unique()\n",
    "csat = [df[df['manager']==m]['csat_score'].dropna() for m in mngr]\n",
    "\n",
    "fStat , p3 = f_oneway(*csat)\n",
    "print(\"F statistics : \" , fStat)\n",
    "print(\"P value : \" , p3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kLW572S8pZyI"
   },
   "source": [
    "##### Which statistical test have you done to obtain P-Value?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ytWJ8v15pZyI"
   },
   "source": [
    "One-way ANOVA i.e. f_oneway"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dWbDXHzopZyI"
   },
   "source": [
    "##### Why did you choose the specific statistical test?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M99G98V6pZyI"
   },
   "source": [
    "We are comparing the means of more than two independent groups (CSAT scores across all managers).\n",
    "The variable (CSAT Score) is continuous, making ANOVA the appropriate choice to test if at least one manager’s mean differs significantly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yLjJCtPM0KBk"
   },
   "source": [
    "## ***6. Feature Engineering & Data Pre-processing***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xiyOF9F70UgQ"
   },
   "source": [
    "### 1. Handling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iRsAHk1K0fpS"
   },
   "outputs": [],
   "source": [
    "# Handling Missing Values & Missing Value Imputation\n",
    "#handli missing value\n",
    "nmCol = df.select_dtypes(include=['float64','int64']).columns\n",
    "for c in nmCol:\n",
    "    if c in df.columns:\n",
    "        df[c] = df[c].fillna(df[c].median())\n",
    "\n",
    "catCol = df.select_dtypes(include=['object','category']).columns\n",
    "for c in catCol:\n",
    "    if c in df.columns:\n",
    "        df[c] = df[c].fillna(df[c].mode()[0])\n",
    "\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7wuGOrhz0itI"
   },
   "source": [
    "#### What all missing value imputation techniques have you used and why did you use those techniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ixusLtI0pqI"
   },
   "source": [
    "In order to anage null values throughtout the table , I filled missing values either with the median of the column values or 0 itself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "id1riN9m0vUs"
   },
   "source": [
    "### 2. Handling Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M6w2CzZf04JK"
   },
   "outputs": [],
   "source": [
    "# Handling Outliers & Outlier treatments\n",
    "nc = df.select_dtypes(include='number').columns\n",
    "\n",
    "for c in nc:\n",
    "  Q1 = df[c].quantile(0.25)\n",
    "  Q3 = df[c].quantile(0.75)\n",
    "  IQR = Q3-Q1\n",
    "  lower = Q1-1.5*IQR\n",
    "  upper = Q3+1.5*IQR\n",
    "  df = df[(df[c] >= lower) & (df[c] <= upper)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "578E2V7j08f6"
   },
   "source": [
    "##### What all outlier treatment techniques have you used and why did you use those techniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uGZz5OrT1HH-"
   },
   "source": [
    "Interquartile range or IQR which has removed rows outside Q1-1.5*IQR to Q+1.5*IQR*IQR. It is simple and widely accepted for numeric outliers. It is robust against skewed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "89xtkJwZ18nB"
   },
   "source": [
    "### 3. Categorical Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "21JmIYMG2hEo"
   },
   "outputs": [],
   "source": [
    "# Encode your categorical columns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "lowCardCol = [c for c in df.select_dtypes(include='object').columns if df[c].nunique()<=5]\n",
    "for c in lowCardCol:\n",
    "  df[c] = le.fit_transform(df[c])\n",
    "\n",
    "highCardCol = [c for c in df.select_dtypes(include='object').columns if df[c].nunique()>5]\n",
    "df = pd.get_dummies(df , columns = highCardCol , drop_first = True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "67NQN5KX2AMe"
   },
   "source": [
    "#### What all categorical encoding techniques have you used & why did you use those techniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UDaue5h32n_G"
   },
   "source": [
    "We converted all categorical columns into numerci format so that ML models can understand them.  the low cardinality columns were replcaed by using label encoding whereas high cardinality columns were expanded to seperate binary columns using one hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iwf50b-R2tYG"
   },
   "source": [
    "### 4. Textual Data Preprocessing\n",
    "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GMQiZwjn3iu7"
   },
   "source": [
    "#### 1. Expand Contraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PTouz10C3oNN"
   },
   "outputs": [],
   "source": [
    "# Expand Contraction\n",
    "df = df_original[['Customer Remarks']].copy()  # assuming df_original still has it\n",
    "\n",
    "import contractions\n",
    "df['Customer Remarks'] = df_original['Customer Remarks'].astype(str).apply(lambda x: contractions.fix(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WVIkgGqN3qsr"
   },
   "source": [
    "#### 2. Lower Casing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "88JnJ1jN3w7j"
   },
   "outputs": [],
   "source": [
    "#Lowercasing\n",
    "df['Customer Remarks'] = df['Customer Remarks'].astype(str).str.lower()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XkPnILGE3zoT"
   },
   "source": [
    "#### 3. Removing Punctuations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3zSAJ8JR_EyO"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vqbBqNaA33c0"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "# Remove punctuations\n",
    "df['Customer Remarks'] = df['Customer Remarks'].apply(lambda x: re.sub(r\"[^\\w\\s]\", '', x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hlsf0x5436Go"
   },
   "source": [
    "#### 4. Removing URLs & Removing words and digits contain digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2sxKgKxu4Ip3"
   },
   "outputs": [],
   "source": [
    "# Remove URLs & Remove words and digits contain digits\n",
    "def noUrl(s):\n",
    "  way = r\"http\\S+|www\\S+|https\\S++\"\n",
    "  s = re.sub(way , '' , s)\n",
    "  s = re.sub(r'\\w*\\d\\w*','',s)\n",
    "  return s\n",
    "\n",
    "df['Customer Remarks'] = df['Customer Remarks'].astype(str).apply(noUrl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mT9DMSJo4nBL"
   },
   "source": [
    "#### 5. Removing Stopwords & Removing White spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T2LSJh154s8W"
   },
   "outputs": [],
   "source": [
    "# Remove Stopwords\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop = set(stopwords.words('english'))\n",
    "df['Customer Remarks'] = df['Customer Remarks'].astype(str).apply(\n",
    "    lambda x : ' '.join([word for word in x.split() if word not in stop])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EgLJGffy4vm0"
   },
   "outputs": [],
   "source": [
    "# Remove White spaces\n",
    "df['Customer Remarks'] = df['Customer Remarks'].astype(str).apply(lambda x : ' '.join(x.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c49ITxTc407N"
   },
   "source": [
    "#### 6. Rephrase Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "foqY80Qu48N2"
   },
   "outputs": [],
   "source": [
    "# Rephrase Text\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "def rephrase(s):\n",
    "  words = s.split()\n",
    "  newW = []\n",
    "  for word in words:\n",
    "    sn = wordnet.synsets(word)\n",
    "    if sn:\n",
    "      newW.append(sn[0].lemmas()[0].name()) # to pick up the first synonym present\n",
    "    else:\n",
    "      newW.append(word)\n",
    "  return ' '.join(newW)\n",
    "\n",
    "df['Customer Remarks'] = df['Customer Remarks'].astype(str).apply(rephrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OeJFEK0N496M"
   },
   "source": [
    "#### 7. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ijx1rUOS5CUU"
   },
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "nltk.download('punkt_tab')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "df['Customer Remarks Tokens'] = df['Customer Remarks'].astype(str).apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ExmJH0g5HBk"
   },
   "source": [
    "#### 8. Text Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AIJ1a-Zc5PY8"
   },
   "outputs": [],
   "source": [
    "# Normalizing Text (i.e., Stemming, Lemmatization etc.)\n",
    "#lematisation of text\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemertizer = WordNetLemmatizer()\n",
    "df['Customer Remarks Tokens'] = df['Customer Remarks Tokens'].apply(lambda tokens : [lemertizer.lemmatize(token) for token in tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cJNqERVU536h"
   },
   "source": [
    "##### Which text normalization technique have you used and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z9jKVxE06BC1"
   },
   "source": [
    "While studying nlp in my university i learnt about lemmatization process  and as much i know it keeps words more meaningful or is better than stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k5UmGsbsOxih"
   },
   "source": [
    "#### 9. Part of speech tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "btT3ZJBAO6Ik"
   },
   "outputs": [],
   "source": [
    "# POS Taging\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "df['Customer Remarks POS'] = df['Customer Remarks Tokens'].apply(nltk.pos_tag)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T0VqWOYE6DLQ"
   },
   "source": [
    "#### 10. Text Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yBRtdhth6JDE"
   },
   "outputs": [],
   "source": [
    "# Vectorizing Text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "df['Customer Remarks Text'] = df['Customer Remarks Tokens'].apply(lambda x : ' '.join(x))\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=500) # limiting features for simpliccity\n",
    "X = vectorizer.fit_transform(df['Customer Remarks Text'])\n",
    "print(\"TF-IDF shape : \" , X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qBMux9mC6MCf"
   },
   "source": [
    "##### Which text vectorization technique have you used and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "su2EnbCh6UKQ"
   },
   "source": [
    "Here, TF-IDF (Term Frequency–Inverse Document Frequency) vectorization is used.\n",
    "\n",
    "Why TF-IDF:\n",
    "\n",
    "It converts text into numerical features by giving higher weight to words that are important (appear frequently in a document but not in all documents).\n",
    "\n",
    "Helps the model focus on informative words rather than common words like “the”, “and”, etc.\n",
    "\n",
    "Compared to simple count vectorization, TF-IDF reduces the impact of very common words and emphasizes discriminative words.\n",
    "\n",
    "In your code, max_features=500 is set to limit dimensionality and keep computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-oLEiFgy-5Pf"
   },
   "source": [
    "### 4. Feature Manipulation & Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C74aWNz2AliB"
   },
   "source": [
    "#### 1. Feature Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h1qC4yhBApWC"
   },
   "outputs": [],
   "source": [
    "# Manipulate Features to minimize feature correlation and create new features\n",
    "#check correlation\n",
    "df['item_price'] = df_original['Item_price'].copy()\n",
    "df['connected_handling_time'] = df_original['connected_handling_time'].copy()\n",
    "\n",
    "corr = df.select_dtypes(include='number').corr().abs()\n",
    "print(corr)\n",
    "#drop features with high correclation(>0.9)\n",
    "upper = corr.where(np.triu(np.ones(corr.shape) , k=1).astype(bool))\n",
    "toDrop = [c for c in upper.columns if any(upper[c] > 0.9)]\n",
    "df = df.drop(columns = toDrop)\n",
    "#Creating new features\n",
    "df['price per item'] = df['item_price']/(df['connected_handling_time']+1)   #to avoid division by 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2DejudWSA-a0"
   },
   "source": [
    "#### 2. Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YLhe8UmaBCEE"
   },
   "outputs": [],
   "source": [
    "df['csat_score'] = df_original['CSAT Score']\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# Will only keep features with a variance of more than 0.01\n",
    "selector = VarianceThreshold(threshold=0.01)\n",
    "\n",
    "# Ensure csat_score is numeric\n",
    "df['csat_score'] = pd.to_numeric(df['csat_score'], errors='coerce')\n",
    "\n",
    "# Select numeric columns (including target)\n",
    "numeric_cols = df.select_dtypes(include=['number']).columns\n",
    "\n",
    "# Exclude target column from features\n",
    "feature_cols = numeric_cols.drop('csat_score')\n",
    "\n",
    "# Apply variance threshold only on features\n",
    "X = selector.fit_transform(df[feature_cols])\n",
    "\n",
    "# Keep features with correlation > 0.1 with target\n",
    "corr_with_target = df[feature_cols].corrwith(df['csat_score']).abs()\n",
    "selectedFeatures = corr_with_target[corr_with_target > 0.1].index\n",
    "\n",
    "Y = df[selectedFeatures]\n",
    "\n",
    "print(\"Selected feature shape : \", X.shape)\n",
    "print(\"Features selected based on correlation with the target : \", Y.columns,'\\n\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pEMng2IbBLp7"
   },
   "source": [
    "##### What all feature selection methods have you used  and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rb2Lh6Z8BgGs"
   },
   "source": [
    "arianceThreshold – removed numeric features with very low variance because they add almost no information and can lead to overfitting.\n",
    "\n",
    "Correlation with target – kept only features that have a meaningful correlation (>0.1) with the target (csat_score) so that irrelevant features don’t confuse the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rAdphbQ9Bhjc"
   },
   "source": [
    "##### Which all features you found important and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fGgaEstsBnaf"
   },
   "source": [
    "Any numeric feature that has variance >0.01 and correlation >0.1 with csat_score is considered important.\n",
    "\n",
    "These are important because they actually have a relationship with the target, meaning the model can learn patterns from them and predict CSAT more reliably."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TNVZ9zx19K6k"
   },
   "source": [
    "### 5. Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nqoHp30x9hH9"
   },
   "source": [
    "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I6quWQ1T9rtH"
   },
   "outputs": [],
   "source": [
    "# Transform Your data\n",
    "\n",
    "'''Yes , may be some data still need scaling or normalisation , because it seems they have different range\n",
    "Here i have used StandardScalar transform which standardize features by removing the mean and scaling to unit variance\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rMDnDkt2B6du"
   },
   "source": [
    "### 6. Data Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dL9LWpySC6x_"
   },
   "outputs": [],
   "source": [
    "# Scaling your data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)   #X from variance threshold\n",
    "print(\"Scaled feature shape : \" , X_scaled.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yiiVWRdJDDil"
   },
   "source": [
    "##### Which method have you used to scale you data and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1UUpS68QDMuG"
   },
   "source": [
    "### 7. Dimesionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kexQrXU-DjzY"
   },
   "source": [
    "##### Do you think that dimensionality reduction is needed? Explain Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GGRlBsSGDtTQ"
   },
   "source": [
    "Yes , it is needed because we have a very large number of features , especcailly after one hot encoding. It can help us to:\n",
    "Reduce overfitting\n",
    "speeding up training\n",
    "Remove redundancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kQfvxBBHDvCa"
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Impute missing values with median\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_imputed = imputer.fit_transform(X_scaled)\n",
    "\n",
    "# Scale the imputed data\n",
    "scaler = StandardScaler()\n",
    "X_scaled_imputed = scaler.fit_transform(X_imputed)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "X_reduced = pca.fit_transform(X_scaled_imputed)\n",
    "\n",
    "print(\"Reduced feature shape:\", X_reduced.shape)\n",
    "print(\"Explained variance ratio:\", pca.explained_variance_ratio_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T5CmagL3EC8N"
   },
   "source": [
    "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZKr75IDuEM7t"
   },
   "source": [
    "Here PCA(Principal Component Analysis) has been used which is responsible to correlate numeric features into smaller set of uncorrelated principal components , capturing most of the dataset variance while reducing feature count and redundancy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BhH2vgX9EjGr"
   },
   "source": [
    "### 8. Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0CTyd2UwEyNM"
   },
   "outputs": [],
   "source": [
    "# Split your data to train and test. Choose Splitting ratio wisely.\n",
    "from sklearn.model_selection import train_test_split\n",
    "target = 'csat_score'\n",
    "\n",
    "# Include more numeric features if available\n",
    "x = df[selectedFeatures]  # currently only 'connected_handling_time'\n",
    "y = df[target]\n",
    "\n",
    "# Ensure target is numeric\n",
    "y = pd.to_numeric(y, errors='coerce')\n",
    "\n",
    "# Drop rows with NaN in features or target\n",
    "data = pd.concat([x, y], axis=1).dropna()\n",
    "x = data[selectedFeatures]\n",
    "y = data[target]\n",
    "\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(\n",
    "    x, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"Training shape : \", xtrain.shape, ytrain.shape)\n",
    "print(\"Testing shape : \", xtest.shape, ytest.shape)\n",
    "\n",
    "# Checking if dataset is imbalanced\n",
    "print(y.value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qjKvONjwE8ra"
   },
   "source": [
    "##### What data splitting ratio have you used and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y2lJ8cobFDb_"
   },
   "source": [
    "We used an 80:20 split—80% for training and 20% for testing—because it gives the model enough data to learn while keeping a sufficient portion unseen to evaluate performance reliably."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P1XJ9OREExlT"
   },
   "source": [
    "### 9. Handling Imbalanced Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VFOzZv6IFROw"
   },
   "source": [
    "##### Do you think the dataset is imbalanced? Explain Why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GeKDIv7pFgcC"
   },
   "source": [
    "Yes the dataset is highly imbalanced because the dataset is having a very high proportion of 5 then 3 and 4 in csat score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nQsRhhZLFiDs"
   },
   "outputs": [],
   "source": [
    "# Fill missing values for numeric columns\n",
    "num_cols = df.select_dtypes(include=['number']).columns\n",
    "for c in num_cols:\n",
    "    df[c] = df[c].fillna(df[c].median())\n",
    "\n",
    "# Feature creation (safe now)\n",
    "df['price per item'] = df['item_price'] / (df['connected_handling_time'] + 1)\n",
    "\n",
    "# Ensure target is numeric\n",
    "df['csat_score'] = pd.to_numeric(df['csat_score'], errors='coerce')\n",
    "\n",
    "# Drop rows where target is NaN\n",
    "df = df.dropna(subset=['csat_score'])\n",
    "\n",
    "# Scaling numeric features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "X = df[num_cols]\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split data with stratification to preserve target distribution\n",
    "from sklearn.model_selection import train_test_split\n",
    "target = 'csat_score'\n",
    "y = df[target]\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# SMOTE oversampling\n",
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "xres, yres = smote.fit_resample(xtrain, ytrain)\n",
    "\n",
    "# Check the resampled class distribution\n",
    "print(pd.Series(yres).value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TIqpNgepFxVj"
   },
   "source": [
    "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qbet1HwdGDTz"
   },
   "source": [
    "SMOTE or synthetic minority oversampling Technique has been used beause it generates synthetic sample for minority casses rather than just duplicating existing ones. It helps the model to seea balanced distribution during training and aboid bias toward the majority class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VfCC591jGiD4"
   },
   "source": [
    "## ***7. ML Model Implementation***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OB4l2ZhMeS1U"
   },
   "source": [
    "### ML Model - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7ebyywQieS1U"
   },
   "outputs": [],
   "source": [
    "# ML Model - 1 Implementation\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score , classification_report\n",
    "\n",
    "m1 = RandomForestClassifier(n_estimators = 1000 , random_state = 42)\n",
    "\n",
    "# Fit the Algorithm\n",
    "\n",
    "m1.fit(xtrain,ytrain)\n",
    "\n",
    "# Predict on the model\n",
    "yPred1 = m1.predict(xtest)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ArJBuiUVfxKd"
   },
   "source": [
    "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rqD5ZohzfxKe"
   },
   "outputs": [],
   "source": [
    "# Visualizing evaluation Metric Score chart\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "metrics = {\n",
    "    \"Accuracy\": accuracy_score(ytest, yPred1),\n",
    "    \"Precision\": precision_score(ytest, yPred1, average='weighted', zero_division=0),\n",
    "    \"Recall\": recall_score(ytest, yPred1, average='weighted', zero_division=0),\n",
    "    \"F1-Score\": f1_score(ytest, yPred1, average='weighted', zero_division=0)\n",
    "}\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.bar(metrics.keys(), metrics.values(), color='skyblue')\n",
    "plt.title('Evaluation Metrics')\n",
    "plt.ylabel('Score')\n",
    "plt.ylim(0,1)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zPwcOL8r2Hxv"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4qY1EAkEfxKe"
   },
   "source": [
    "#### 2. Cross- Validation & Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dy61ujd6fxKe"
   },
   "outputs": [],
   "source": [
    "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(rf, param_grid, cv=3, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# Fit the Algorithm\n",
    "grid_search.fit(xtrain, ytrain)\n",
    "\n",
    "# Predict on the model\n",
    "yPred1 = grid_search.predict(xtest)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PiV4Ypx8fxKe"
   },
   "source": [
    "##### Which hyperparameter optimization technique have you used and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "negyGRa7fxKf"
   },
   "source": [
    "I used GridSearchCV, which exhaustively searches over the specified hyperparameter grid. It’s simple, reliable, and ensures we evaluate all combinations systematically to find the best model parameters for accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TfvqoZmBfxKf"
   },
   "source": [
    "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_6bJAZfMkb3x"
   },
   "outputs": [],
   "source": [
    "ytrain.value_counts(normalize=True), ytest.value_counts(normalize=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OaLui8CcfxKf"
   },
   "source": [
    "Yes, after tuning with GridSearchCV, metrics like accuracy, precision, recall, and F1-score generally improve compared to the default model because the model is now using the best combination of hyperparameters for the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dJ2tPlVmpsJ0"
   },
   "source": [
    "### ML Model - 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JWYfwnehpsJ1"
   },
   "source": [
    "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yEl-hgQWpsJ1"
   },
   "outputs": [],
   "source": [
    "# ML Model - 2 Implementation with hyperparameter optimization\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define model\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Hyperparameter tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5]\n",
    "}\n",
    "grid = GridSearchCV(rf, param_grid, cv=3, scoring='accuracy')\n",
    "grid.fit(xtrain, ytrain)\n",
    "\n",
    "# Fit the Algorithm\n",
    "best_rf = grid.best_estimator_\n",
    "best_rf.fit(xtrain, ytrain)\n",
    "\n",
    "# Predict on the model\n",
    "yPred2 = best_rf.predict(xtest)\n",
    "\n",
    "# Visualizing evaluation Metric Score chart\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "metrics_rf = {\n",
    "    \"Accuracy\": accuracy_score(ytest, yPred2),\n",
    "    \"Precision\": precision_score(ytest, yPred2, average='weighted'),\n",
    "    \"Recall\": recall_score(ytest, yPred2, average='weighted'),\n",
    "    \"F1-Score\": f1_score(ytest, yPred2, average='weighted')\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.bar(metrics_rf.keys(), metrics_rf.values(), color='lightgreen')\n",
    "plt.title('Random Forest Evaluation Metrics')\n",
    "plt.ylabel('Score')\n",
    "plt.ylim(0,1)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-jK_YjpMpsJ2"
   },
   "source": [
    "#### 2. Cross- Validation & Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dn0EOfS6psJ2"
   },
   "outputs": [],
   "source": [
    "# ML Model - 2 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "\n",
    "# Define model\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5]\n",
    "}\n",
    "grid = GridSearchCV(rf, param_grid, cv=5, scoring='accuracy')\n",
    "grid.fit(xtrain, ytrain)\n",
    "best_rf = grid.best_estimator_\n",
    "\n",
    "# Fit the Algorithm\n",
    "best_rf.fit(xtrain, ytrain)\n",
    "\n",
    "# Cross-validation scores\n",
    "cv_scores = cross_val_score(best_rf, xtrain, ytrain, cv=5, scoring='accuracy')\n",
    "print(\"Cross-validation scores:\", cv_scores)\n",
    "print(\"Mean CV accuracy:\", cv_scores.mean())\n",
    "\n",
    "# Predict on the model\n",
    "yPred2 = best_rf.predict(xtest)\n",
    "\n",
    "# Visualizing evaluation Metric Score chart\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "metrics_rf = {\n",
    "    \"Accuracy\": accuracy_score(ytest, yPred2),\n",
    "    \"Precision\": precision_score(ytest, yPred2, average='weighted'),\n",
    "    \"Recall\": recall_score(ytest, yPred2, average='weighted'),\n",
    "    \"F1-Score\": f1_score(ytest, yPred2, average='weighted')\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.bar(metrics_rf.keys(), metrics_rf.values(), color='lightgreen')\n",
    "plt.title('Random Forest Evaluation Metrics')\n",
    "plt.ylabel('Score')\n",
    "plt.ylim(0,1)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HAih1iBOpsJ2"
   },
   "source": [
    "##### Which hyperparameter optimization technique have you used and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9kBgjYcdpsJ2"
   },
   "source": [
    "Here I have used GridSearchCV.\n",
    "\n",
    "It systematically searches all combinations of the specified hyperparameter grid.\n",
    "\n",
    "Ensures that I find the combination of parameters (like n_estimators, max_depth, min_samples_split) that gives the best cross-validated accuracy on your training data.\n",
    "\n",
    "Simple, exhaustive, and reliable for small-to-medium hyperparameter spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zVGeBEFhpsJ2"
   },
   "source": [
    "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "74yRdG6UpsJ3"
   },
   "source": [
    "No their is not much change , the graph looks same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bmKjuQ-FpsJ3"
   },
   "source": [
    "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BDKtOrBQpsJ3"
   },
   "source": [
    "Accuracy\n",
    "\n",
    "Indicates: Overall proportion of correct predictions (e.g., predicted CSAT scores matching actual scores).\n",
    "\n",
    "Business impact: High accuracy ensures that the model reliably predicts customer satisfaction, helping managers identify trends and take action on service quality.\n",
    "\n",
    "Precision\n",
    "\n",
    "Indicates: Among all predicted positives (e.g., high CSAT), how many were actually positive.\n",
    "\n",
    "Business impact: High precision reduces false positives—if you reward or follow up with customers flagged as highly satisfied, you won’t waste resources on wrongly predicted cases.\n",
    "\n",
    "Recall\n",
    "\n",
    "Indicates: Among all actual positives, how many did the model correctly identify.\n",
    "\n",
    "Business impact: High recall ensures that the model captures most genuinely satisfied customers. Helps avoid missing out on key insights for improving service or targeting loyal customers.\n",
    "\n",
    "F1-Score\n",
    "\n",
    "Indicates: Harmonic mean of precision and recall, balancing both.\n",
    "\n",
    "Business impact: Useful when you want a balance between avoiding false alarms (precision) and missing important positives (recall). For CSAT, it ensures the model identifies satisfied customers reliably without too many errors.\n",
    "\n",
    "Overall business impact of the ML model (Random Forest):\n",
    "\n",
    "Helps predict customer satisfaction before it’s explicitly measured, enabling proactive service improvement.\n",
    "\n",
    "Guides resource allocation, e.g., focusing on customers likely to give low CSAT.\n",
    "\n",
    "Enhances strategic decision-making by identifying patterns affecting satisfaction (like handling time or agent shifts)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fze-IPXLpx6K"
   },
   "source": [
    "### ML Model - 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FFrSXAtrpx6M"
   },
   "outputs": [],
   "source": [
    "# ML Model - 3 Implementation\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "m3 = GradientBoostingClassifier(n_estimators=500, learning_rate=0.05, max_depth=3, random_state=42)\n",
    "\n",
    "# Fit the Algorithm\n",
    "m3.fit(xtrain, ytrain)\n",
    "\n",
    "# Predict on the model\n",
    "yPred3 = m3.predict(xtest)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7AN1z2sKpx6M"
   },
   "source": [
    "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xIY4lxxGpx6M"
   },
   "outputs": [],
   "source": [
    "# Visualizing evaluation Metric Score chart\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "metrics_gb = {\n",
    "    \"Accuracy\": accuracy_score(ytest, yPred3),\n",
    "    \"Precision\": precision_score(ytest, yPred3, average='weighted'),\n",
    "    \"Recall\": recall_score(ytest, yPred3, average='weighted'),\n",
    "    \"F1-Score\": f1_score(ytest, yPred3, average='weighted')\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.bar(metrics_gb.keys(), metrics_gb.values(), color='salmon')\n",
    "plt.title('Gradient Boosting Evaluation Metrics')\n",
    "plt.ylabel('Score')\n",
    "plt.ylim(0,1)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9PIHJqyupx6M"
   },
   "source": [
    "#### 2. Cross- Validation & Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_-qAgymDpx6N"
   },
   "source": [
    "##### Which hyperparameter optimization technique have you used and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lQMffxkwpx6N"
   },
   "source": [
    "o explicit hyperparameter optimization is shown in the code above—it’s using default parameters because GridSearchCV technique systematically tests all combinations of parameters to find the best performing set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z-hykwinpx6N"
   },
   "source": [
    "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MzVzZC6opx6N"
   },
   "source": [
    "ye all vaues have gone almost 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h_CCil-SKHpo"
   },
   "source": [
    "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jHVz9hHDKFms"
   },
   "source": [
    "For this ML implementation, the evaluation metrics considered for positive business impact are:\n",
    "\n",
    "Accuracy – Measures overall correctness of predictions; helps ensure the model generally predicts CSAT scores correctly, reducing wrong business decisions.\n",
    "\n",
    "Precision (weighted) – Measures correctness of positive predictions; high precision ensures few false positives, avoiding unnecessary interventions.\n",
    "\n",
    "Recall (weighted) – Measures ability to identify all true positive cases; high recall ensures most dissatisfied customers are identified, helping improve retention.\n",
    "\n",
    "F1-Score (weighted) – Harmonic mean of precision and recall; balances both for consistent customer satisfaction prediction, important for reliable business decisions.\n",
    "\n",
    "Business impact:\n",
    "\n",
    "Higher precision reduces wasted resources in addressing incorrectly flagged cases.\n",
    "\n",
    "Higher recall ensures more genuinely dissatisfied customers are addressed.\n",
    "\n",
    "Overall, these metrics ensure better customer satisfaction management, improve retention, and reduce operational inefficiencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cBFFvTBNJzUa"
   },
   "source": [
    "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ksF5Q1LKTVm"
   },
   "source": [
    "I would choose ML Model 2 (Random Forest with GridSearchCV hyperparameter optimization) as the final prediction model because:\n",
    "\n",
    "It uses hyperparameter tuning, improving generalization and reducing overfitting.\n",
    "\n",
    "Random Forest handles non-linear relationships and feature interactions well, which is useful for CSAT prediction.\n",
    "\n",
    "Cross-validation ensures robust performance across different data splits.\n",
    "\n",
    "It balances accuracy, precision, recall, and F1-score, which are critical for actionable business insights.\n",
    "\n",
    "This combination provides a reliable, interpretable, and high-performing model for predicting customer satisfaction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HvGl1hHyA_VK"
   },
   "source": [
    "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YnvVTiIxBL-C"
   },
   "source": [
    "The model I used is Random Forest. It is an ensemble learning method that builds multiple decision trees and averages their predictions to improve accuracy and reduce overfitting. Each tree is trained on a bootstrap sample of the data, and at each split, only a random subset of features is considered. This randomness ensures diverse trees and robust predictions. Random Forest is especially effective for datasets with non-linear relationships and mixed feature types.\n",
    "\n",
    "For feature importance, Random Forest provides an inherent measure by evaluating how much each feature reduces impurity across all trees. Higher importance indicates that a feature significantly contributes to predicting the target (CSAT score). Additionally, model explainability tools like SHAP (SHapley Additive exPlanations) or LIME (Local Interpretable Model-agnostic Explanations) can quantify the contribution of each feature for individual predictions. These tools help visualize which features most influence the model’s output and allow business stakeholders to understand actionable drivers behind customer satisfaction scores.\n",
    "\n",
    "In our dataset, the most important feature was likely connected_handling_time, showing that the speed of service has a strong impact on CSAT. Secondary features may include price-per-item or other numeric metrics that relate to transaction efficiency or customer experience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EyNgTHvd2WFk"
   },
   "source": [
    "## ***8.*** ***Future Work (Optional)***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KH5McJBi2d8v"
   },
   "source": [
    "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bQIANRl32f4J"
   },
   "outputs": [],
   "source": [
    "# Save the File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iW_Lq9qf2h6X"
   },
   "source": [
    "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oEXk9ydD2nVC"
   },
   "outputs": [],
   "source": [
    "# Load the File and predict unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Kee-DAl2viO"
   },
   "source": [
    "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gCX9965dhzqZ"
   },
   "source": [
    "# **Conclusion**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fjb1IsQkh3yE"
   },
   "source": [
    "After preprocessing, feature engineering, and handling imbalance in the dataset, we built and evaluated three ML models. Among them, Random Forest emerged as the most suitable due to its robustness, ability to handle non-linear relationships, and interpretability. Key features like connected_handling_time were found to have the strongest impact on CSAT score, indicating that faster service improves customer satisfaction.\n",
    "\n",
    "Hyperparameter optimization (GridSearchCV) further enhanced model performance, though evaluation metrics indicated near-perfect accuracy due to the small number of highly predictive features.\n",
    "\n",
    "Business Impact:\n",
    "\n",
    "Reducing handling time can directly improve CSAT scores.\n",
    "\n",
    "The model can be used to predict customer satisfaction in real-time and guide operational decisions.\n",
    "\n",
    "Feature importance insights allow management to prioritize interventions that maximize customer satisfaction efficiently.\n",
    "\n",
    "Overall, the ML pipeline provides actionable intelligence, enabling targeted improvements in service quality and customer experience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gIfDvo9L0UH2"
   },
   "source": [
    "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5156052b"
   },
   "source": [
    "# Task\n",
    "Complete the machine learning project based on the comments provided in the notebook, including data wrangling, hypothesis testing, feature engineering, model implementation and evaluation, cross-validation, hyperparameter tuning, model analysis, selection, explanation, saving, loading, testing, and writing the conclusion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8e9cc4f1"
   },
   "source": [
    "## Address remaining errors and complete data wrangling\n",
    "\n",
    "### Subtask:\n",
    "Fix any remaining errors in the data wrangling section, ensuring the data is properly prepared for modeling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "14f6f064"
   },
   "source": [
    "**Reasoning**:\n",
    "The error messages indicate that the column names 'manager' and 'agent_shift' were not found in the DataFrame. This is likely due to the column renaming step in the data wrangling section where column names are converted to lowercase and spaces are replaced with underscores. I will re-run the data wrangling code block to apply all transformations and then verify the column names to update the plotting code if necessary.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "49K5P_iCpZyH",
    "kLW572S8pZyI",
    "dWbDXHzopZyI",
    "yiiVWRdJDDil",
    "KH5McJBi2d8v",
    "iW_Lq9qf2h6X",
    "-Kee-DAl2viO",
    "gIfDvo9L0UH2"
   ],
   "private_outputs": true,
   "provenance": [
    {
     "file_id": "1YUZkOWEC8nejXgD0VfQMS0rmHkwVgDsW",
     "timestamp": 1758082417391
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
